{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "try_tinyML.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOxill02FxCXjGv9WYNcoak",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tantatorn-dev/try_tinyML/blob/master/try_tinyML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r82A2U6MEhML",
        "colab_type": "text"
      },
      "source": [
        "# Create a model\n",
        "At first, we must create a model in Python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVGJvDP8ApJ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def get_model():\n",
        "    SAMPLES = 1000\n",
        "    np.random.seed(1337)\n",
        "    x_values = np.random.uniform(low=0, high=2*math.pi, size=SAMPLES)\n",
        "    # shuffle and add noise\n",
        "    np.random.shuffle(x_values)\n",
        "    y_values = np.sin(x_values)\n",
        "    y_values += 0.1 * np.random.randn(*y_values.shape)\n",
        "\n",
        "    # split into train, validation, test\n",
        "    TRAIN_SPLIT =  int(0.6 * SAMPLES)\n",
        "    TEST_SPLIT = int(0.2 * SAMPLES + TRAIN_SPLIT)\n",
        "    x_train, x_test, x_validate = np.split(x_values, [TRAIN_SPLIT, TEST_SPLIT])\n",
        "    y_train, y_test, y_validate = np.split(y_values, [TRAIN_SPLIT, TEST_SPLIT])\n",
        "\n",
        "    # create a NN with 2 layers of 16 neurons\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(16, activation='relu', input_shape=(1,)))\n",
        "    model.add(layers.Dense(16, activation='relu'))\n",
        "    model.add(layers.Dense(1))\n",
        "    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
        "    model.fit(x_train, y_train, epochs=200, batch_size=16,\n",
        "                        validation_data=(x_validate, y_validate))\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_UkUE24H8VW",
        "colab_type": "text"
      },
      "source": [
        "# Export a model for an embedded device\n",
        "We'll use TinyML for this.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLL3kZJfF8yP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 923
        },
        "outputId": "4cace036-36fc-412b-d31c-2aa3b4d97319"
      },
      "source": [
        "!pip install tinymlgen"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tinymlgen\n",
            "  Downloading https://files.pythonhosted.org/packages/58/02/1425d90f1489dbab7dbc74882f0070b75c109bb83b785d817c023e1ccd62/tinymlgen-0.2.tar.gz\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (from tinymlgen) (2.3.0)\n",
            "Collecting hexdump\n",
            "  Downloading https://files.pythonhosted.org/packages/55/b3/279b1d57fa3681725d0db8820405cdcb4e62a9239c205e4ceac4391c78e4/hexdump-3.3.zip\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->tinymlgen) (2.10.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->tinymlgen) (2.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->tinymlgen) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow->tinymlgen) (0.35.1)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->tinymlgen) (1.4.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->tinymlgen) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->tinymlgen) (3.12.4)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->tinymlgen) (1.12.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow->tinymlgen) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->tinymlgen) (0.8.1)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow->tinymlgen) (1.6.3)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow->tinymlgen) (0.3.3)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->tinymlgen) (2.3.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->tinymlgen) (1.31.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->tinymlgen) (3.3.0)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->tinymlgen) (1.18.5)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->tinymlgen) (1.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow->tinymlgen) (49.6.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->tinymlgen) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->tinymlgen) (1.7.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->tinymlgen) (0.4.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->tinymlgen) (1.17.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->tinymlgen) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->tinymlgen) (3.2.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow->tinymlgen) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->tinymlgen) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->tinymlgen) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->tinymlgen) (4.1.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow->tinymlgen) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow->tinymlgen) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow->tinymlgen) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow->tinymlgen) (2020.6.20)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow->tinymlgen) (1.7.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow->tinymlgen) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->tinymlgen) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow->tinymlgen) (3.1.0)\n",
            "Building wheels for collected packages: tinymlgen, hexdump\n",
            "  Building wheel for tinymlgen (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinymlgen: filename=tinymlgen-0.2-cp36-none-any.whl size=2244 sha256=dcba33091935d2f0dad34bf981ab97a16bed90ad268e7b41c0847883342cee71\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/84/4d/3c1a67c7b9483e296ff1b2f4a13e2f800ca2a6093b741450d2\n",
            "  Building wheel for hexdump (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hexdump: filename=hexdump-3.3-cp36-none-any.whl size=8914 sha256=13fff15508361b1fb489e7eac7809d4eff9bc757da5dc38eec1b92a9fef40ee6\n",
            "  Stored in directory: /root/.cache/pip/wheels/d5/d1/f2/c8183b5863b3df595c2eeafd8e015a43dae13d403a959467c6\n",
            "Successfully built tinymlgen hexdump\n",
            "Installing collected packages: hexdump, tinymlgen\n",
            "Successfully installed hexdump-3.3 tinymlgen-0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00lWC7ZY-XmT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d05c20d8-a51d-481e-8d84-53b3ac6e17dc"
      },
      "source": [
        "from tinymlgen import port\n",
        "\n",
        "model = get_model()\n",
        "c_code = port(model, pretty_print=True)\n",
        "print(c_code)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.9680 - mae: 0.8492 - val_loss: 0.6965 - val_mae: 0.7709\n",
            "Epoch 2/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.4944 - mae: 0.6277 - val_loss: 0.4953 - val_mae: 0.6238\n",
            "Epoch 3/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.3849 - mae: 0.5399 - val_loss: 0.4102 - val_mae: 0.5567\n",
            "Epoch 4/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.3251 - mae: 0.4942 - val_loss: 0.3505 - val_mae: 0.5195\n",
            "Epoch 5/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2746 - mae: 0.4564 - val_loss: 0.2889 - val_mae: 0.4671\n",
            "Epoch 6/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2288 - mae: 0.4174 - val_loss: 0.2379 - val_mae: 0.4307\n",
            "Epoch 7/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1923 - mae: 0.3855 - val_loss: 0.1975 - val_mae: 0.3906\n",
            "Epoch 8/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1668 - mae: 0.3583 - val_loss: 0.1710 - val_mae: 0.3649\n",
            "Epoch 9/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1497 - mae: 0.3390 - val_loss: 0.1559 - val_mae: 0.3469\n",
            "Epoch 10/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1377 - mae: 0.3215 - val_loss: 0.1346 - val_mae: 0.3190\n",
            "Epoch 11/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1289 - mae: 0.3081 - val_loss: 0.1253 - val_mae: 0.3062\n",
            "Epoch 12/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1220 - mae: 0.2977 - val_loss: 0.1168 - val_mae: 0.2935\n",
            "Epoch 13/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1156 - mae: 0.2857 - val_loss: 0.1105 - val_mae: 0.2826\n",
            "Epoch 14/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1102 - mae: 0.2750 - val_loss: 0.1097 - val_mae: 0.2793\n",
            "Epoch 15/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1054 - mae: 0.2668 - val_loss: 0.1061 - val_mae: 0.2698\n",
            "Epoch 16/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1009 - mae: 0.2584 - val_loss: 0.0945 - val_mae: 0.2545\n",
            "Epoch 17/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0967 - mae: 0.2501 - val_loss: 0.0908 - val_mae: 0.2468\n",
            "Epoch 18/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0953 - mae: 0.2461 - val_loss: 0.0873 - val_mae: 0.2403\n",
            "Epoch 19/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0907 - mae: 0.2401 - val_loss: 0.0814 - val_mae: 0.2253\n",
            "Epoch 20/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0867 - mae: 0.2296 - val_loss: 0.0854 - val_mae: 0.2357\n",
            "Epoch 21/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0843 - mae: 0.2263 - val_loss: 0.0803 - val_mae: 0.2265\n",
            "Epoch 22/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0807 - mae: 0.2213 - val_loss: 0.0722 - val_mae: 0.2053\n",
            "Epoch 23/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0774 - mae: 0.2117 - val_loss: 0.0753 - val_mae: 0.2161\n",
            "Epoch 24/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0761 - mae: 0.2115 - val_loss: 0.0672 - val_mae: 0.2011\n",
            "Epoch 25/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0734 - mae: 0.2053 - val_loss: 0.0670 - val_mae: 0.1998\n",
            "Epoch 26/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0702 - mae: 0.2010 - val_loss: 0.0614 - val_mae: 0.1886\n",
            "Epoch 27/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0673 - mae: 0.1951 - val_loss: 0.0588 - val_mae: 0.1838\n",
            "Epoch 28/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0652 - mae: 0.1908 - val_loss: 0.0571 - val_mae: 0.1816\n",
            "Epoch 29/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0620 - mae: 0.1853 - val_loss: 0.0618 - val_mae: 0.1900\n",
            "Epoch 30/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0595 - mae: 0.1832 - val_loss: 0.0548 - val_mae: 0.1706\n",
            "Epoch 31/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0586 - mae: 0.1774 - val_loss: 0.0499 - val_mae: 0.1647\n",
            "Epoch 32/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0554 - mae: 0.1742 - val_loss: 0.0478 - val_mae: 0.1655\n",
            "Epoch 33/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0524 - mae: 0.1693 - val_loss: 0.0460 - val_mae: 0.1609\n",
            "Epoch 34/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0505 - mae: 0.1671 - val_loss: 0.0421 - val_mae: 0.1522\n",
            "Epoch 35/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0474 - mae: 0.1619 - val_loss: 0.0401 - val_mae: 0.1420\n",
            "Epoch 36/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0457 - mae: 0.1576 - val_loss: 0.0391 - val_mae: 0.1473\n",
            "Epoch 37/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0440 - mae: 0.1540 - val_loss: 0.0372 - val_mae: 0.1443\n",
            "Epoch 38/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0426 - mae: 0.1549 - val_loss: 0.0372 - val_mae: 0.1503\n",
            "Epoch 39/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0394 - mae: 0.1484 - val_loss: 0.0324 - val_mae: 0.1322\n",
            "Epoch 40/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0378 - mae: 0.1465 - val_loss: 0.0390 - val_mae: 0.1520\n",
            "Epoch 41/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0372 - mae: 0.1458 - val_loss: 0.0314 - val_mae: 0.1380\n",
            "Epoch 42/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0344 - mae: 0.1394 - val_loss: 0.0311 - val_mae: 0.1355\n",
            "Epoch 43/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0337 - mae: 0.1390 - val_loss: 0.0277 - val_mae: 0.1290\n",
            "Epoch 44/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0321 - mae: 0.1356 - val_loss: 0.0400 - val_mae: 0.1555\n",
            "Epoch 45/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0311 - mae: 0.1326 - val_loss: 0.0294 - val_mae: 0.1397\n",
            "Epoch 46/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0295 - mae: 0.1314 - val_loss: 0.0252 - val_mae: 0.1098\n",
            "Epoch 47/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0282 - mae: 0.1271 - val_loss: 0.0257 - val_mae: 0.1274\n",
            "Epoch 48/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0270 - mae: 0.1256 - val_loss: 0.0217 - val_mae: 0.1109\n",
            "Epoch 49/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0259 - mae: 0.1236 - val_loss: 0.0220 - val_mae: 0.1130\n",
            "Epoch 50/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0245 - mae: 0.1198 - val_loss: 0.0230 - val_mae: 0.1232\n",
            "Epoch 51/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0248 - mae: 0.1216 - val_loss: 0.0228 - val_mae: 0.1190\n",
            "Epoch 52/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0237 - mae: 0.1197 - val_loss: 0.0182 - val_mae: 0.1057\n",
            "Epoch 53/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0226 - mae: 0.1156 - val_loss: 0.0186 - val_mae: 0.1006\n",
            "Epoch 54/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0219 - mae: 0.1139 - val_loss: 0.0166 - val_mae: 0.1004\n",
            "Epoch 55/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0211 - mae: 0.1133 - val_loss: 0.0190 - val_mae: 0.1068\n",
            "Epoch 56/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0212 - mae: 0.1138 - val_loss: 0.0159 - val_mae: 0.0928\n",
            "Epoch 57/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0202 - mae: 0.1115 - val_loss: 0.0155 - val_mae: 0.0982\n",
            "Epoch 58/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0202 - mae: 0.1100 - val_loss: 0.0141 - val_mae: 0.0920\n",
            "Epoch 59/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0189 - mae: 0.1078 - val_loss: 0.0155 - val_mae: 0.0915\n",
            "Epoch 60/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0193 - mae: 0.1090 - val_loss: 0.0145 - val_mae: 0.0896\n",
            "Epoch 61/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0186 - mae: 0.1057 - val_loss: 0.0160 - val_mae: 0.0981\n",
            "Epoch 62/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0182 - mae: 0.1072 - val_loss: 0.0156 - val_mae: 0.0909\n",
            "Epoch 63/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0166 - mae: 0.1031 - val_loss: 0.0174 - val_mae: 0.1071\n",
            "Epoch 64/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0173 - mae: 0.1033 - val_loss: 0.0161 - val_mae: 0.0928\n",
            "Epoch 65/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0178 - mae: 0.1042 - val_loss: 0.0123 - val_mae: 0.0893\n",
            "Epoch 66/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0170 - mae: 0.1028 - val_loss: 0.0123 - val_mae: 0.0900\n",
            "Epoch 67/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0164 - mae: 0.1001 - val_loss: 0.0122 - val_mae: 0.0847\n",
            "Epoch 68/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0161 - mae: 0.1001 - val_loss: 0.0131 - val_mae: 0.0927\n",
            "Epoch 69/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0164 - mae: 0.1001 - val_loss: 0.0151 - val_mae: 0.0995\n",
            "Epoch 70/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0161 - mae: 0.1011 - val_loss: 0.0126 - val_mae: 0.0866\n",
            "Epoch 71/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0159 - mae: 0.0995 - val_loss: 0.0133 - val_mae: 0.0924\n",
            "Epoch 72/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0154 - mae: 0.0973 - val_loss: 0.0127 - val_mae: 0.0903\n",
            "Epoch 73/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0155 - mae: 0.0975 - val_loss: 0.0133 - val_mae: 0.0926\n",
            "Epoch 74/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0154 - mae: 0.0986 - val_loss: 0.0135 - val_mae: 0.0872\n",
            "Epoch 75/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0153 - mae: 0.0965 - val_loss: 0.0119 - val_mae: 0.0862\n",
            "Epoch 76/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0150 - mae: 0.0969 - val_loss: 0.0119 - val_mae: 0.0836\n",
            "Epoch 77/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0147 - mae: 0.0955 - val_loss: 0.0104 - val_mae: 0.0814\n",
            "Epoch 78/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0140 - mae: 0.0933 - val_loss: 0.0116 - val_mae: 0.0856\n",
            "Epoch 79/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0142 - mae: 0.0948 - val_loss: 0.0100 - val_mae: 0.0793\n",
            "Epoch 80/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0146 - mae: 0.0956 - val_loss: 0.0113 - val_mae: 0.0850\n",
            "Epoch 81/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0144 - mae: 0.0950 - val_loss: 0.0130 - val_mae: 0.0908\n",
            "Epoch 82/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0141 - mae: 0.0946 - val_loss: 0.0109 - val_mae: 0.0844\n",
            "Epoch 83/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0138 - mae: 0.0928 - val_loss: 0.0125 - val_mae: 0.0862\n",
            "Epoch 84/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0141 - mae: 0.0946 - val_loss: 0.0131 - val_mae: 0.0894\n",
            "Epoch 85/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0140 - mae: 0.0943 - val_loss: 0.0105 - val_mae: 0.0814\n",
            "Epoch 86/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0138 - mae: 0.0939 - val_loss: 0.0116 - val_mae: 0.0841\n",
            "Epoch 87/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0133 - mae: 0.0921 - val_loss: 0.0104 - val_mae: 0.0815\n",
            "Epoch 88/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0130 - mae: 0.0912 - val_loss: 0.0116 - val_mae: 0.0842\n",
            "Epoch 89/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0135 - mae: 0.0924 - val_loss: 0.0113 - val_mae: 0.0849\n",
            "Epoch 90/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0135 - mae: 0.0933 - val_loss: 0.0162 - val_mae: 0.1032\n",
            "Epoch 91/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0138 - mae: 0.0924 - val_loss: 0.0106 - val_mae: 0.0816\n",
            "Epoch 92/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0135 - mae: 0.0930 - val_loss: 0.0129 - val_mae: 0.0907\n",
            "Epoch 93/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0139 - mae: 0.0945 - val_loss: 0.0124 - val_mae: 0.0869\n",
            "Epoch 94/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0133 - mae: 0.0921 - val_loss: 0.0103 - val_mae: 0.0807\n",
            "Epoch 95/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0129 - mae: 0.0907 - val_loss: 0.0093 - val_mae: 0.0755\n",
            "Epoch 96/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0129 - mae: 0.0903 - val_loss: 0.0123 - val_mae: 0.0884\n",
            "Epoch 97/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0137 - mae: 0.0925 - val_loss: 0.0114 - val_mae: 0.0843\n",
            "Epoch 98/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0129 - mae: 0.0913 - val_loss: 0.0158 - val_mae: 0.0996\n",
            "Epoch 99/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0131 - mae: 0.0916 - val_loss: 0.0141 - val_mae: 0.0943\n",
            "Epoch 100/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0130 - mae: 0.0910 - val_loss: 0.0100 - val_mae: 0.0794\n",
            "Epoch 101/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0132 - mae: 0.0914 - val_loss: 0.0143 - val_mae: 0.0960\n",
            "Epoch 102/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0133 - mae: 0.0921 - val_loss: 0.0097 - val_mae: 0.0769\n",
            "Epoch 103/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0125 - mae: 0.0904 - val_loss: 0.0128 - val_mae: 0.0903\n",
            "Epoch 104/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0130 - mae: 0.0909 - val_loss: 0.0116 - val_mae: 0.0851\n",
            "Epoch 105/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0131 - mae: 0.0920 - val_loss: 0.0088 - val_mae: 0.0719\n",
            "Epoch 106/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0132 - mae: 0.0917 - val_loss: 0.0135 - val_mae: 0.0919\n",
            "Epoch 107/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0127 - mae: 0.0902 - val_loss: 0.0111 - val_mae: 0.0828\n",
            "Epoch 108/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0129 - mae: 0.0890 - val_loss: 0.0135 - val_mae: 0.0926\n",
            "Epoch 109/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0122 - mae: 0.0886 - val_loss: 0.0107 - val_mae: 0.0816\n",
            "Epoch 110/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0125 - mae: 0.0880 - val_loss: 0.0137 - val_mae: 0.0944\n",
            "Epoch 111/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0127 - mae: 0.0895 - val_loss: 0.0116 - val_mae: 0.0837\n",
            "Epoch 112/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0129 - mae: 0.0913 - val_loss: 0.0178 - val_mae: 0.1024\n",
            "Epoch 113/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0124 - mae: 0.0882 - val_loss: 0.0095 - val_mae: 0.0774\n",
            "Epoch 114/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0124 - mae: 0.0884 - val_loss: 0.0106 - val_mae: 0.0814\n",
            "Epoch 115/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0125 - mae: 0.0889 - val_loss: 0.0101 - val_mae: 0.0783\n",
            "Epoch 116/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0131 - mae: 0.0912 - val_loss: 0.0159 - val_mae: 0.0979\n",
            "Epoch 117/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0125 - mae: 0.0897 - val_loss: 0.0129 - val_mae: 0.0902\n",
            "Epoch 118/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0130 - mae: 0.0891 - val_loss: 0.0088 - val_mae: 0.0728\n",
            "Epoch 119/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0132 - mae: 0.0925 - val_loss: 0.0091 - val_mae: 0.0747\n",
            "Epoch 120/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0126 - mae: 0.0889 - val_loss: 0.0114 - val_mae: 0.0836\n",
            "Epoch 121/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0127 - mae: 0.0908 - val_loss: 0.0113 - val_mae: 0.0819\n",
            "Epoch 122/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0124 - mae: 0.0884 - val_loss: 0.0106 - val_mae: 0.0813\n",
            "Epoch 123/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0119 - mae: 0.0879 - val_loss: 0.0098 - val_mae: 0.0767\n",
            "Epoch 124/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0125 - mae: 0.0891 - val_loss: 0.0112 - val_mae: 0.0850\n",
            "Epoch 125/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0122 - mae: 0.0884 - val_loss: 0.0094 - val_mae: 0.0752\n",
            "Epoch 126/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0131 - mae: 0.0926 - val_loss: 0.0101 - val_mae: 0.0793\n",
            "Epoch 127/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0127 - mae: 0.0906 - val_loss: 0.0095 - val_mae: 0.0756\n",
            "Epoch 128/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0124 - mae: 0.0896 - val_loss: 0.0088 - val_mae: 0.0721\n",
            "Epoch 129/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0126 - mae: 0.0897 - val_loss: 0.0123 - val_mae: 0.0884\n",
            "Epoch 130/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0122 - mae: 0.0884 - val_loss: 0.0091 - val_mae: 0.0735\n",
            "Epoch 131/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0123 - mae: 0.0892 - val_loss: 0.0099 - val_mae: 0.0766\n",
            "Epoch 132/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0122 - mae: 0.0887 - val_loss: 0.0094 - val_mae: 0.0757\n",
            "Epoch 133/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0123 - mae: 0.0898 - val_loss: 0.0142 - val_mae: 0.0931\n",
            "Epoch 134/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0122 - mae: 0.0890 - val_loss: 0.0130 - val_mae: 0.0886\n",
            "Epoch 135/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0123 - mae: 0.0895 - val_loss: 0.0170 - val_mae: 0.1028\n",
            "Epoch 136/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0125 - mae: 0.0910 - val_loss: 0.0101 - val_mae: 0.0799\n",
            "Epoch 137/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0123 - mae: 0.0883 - val_loss: 0.0108 - val_mae: 0.0806\n",
            "Epoch 138/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0122 - mae: 0.0886 - val_loss: 0.0134 - val_mae: 0.0915\n",
            "Epoch 139/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0126 - mae: 0.0897 - val_loss: 0.0144 - val_mae: 0.0943\n",
            "Epoch 140/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0122 - mae: 0.0893 - val_loss: 0.0144 - val_mae: 0.0952\n",
            "Epoch 141/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0130 - mae: 0.0915 - val_loss: 0.0088 - val_mae: 0.0718\n",
            "Epoch 142/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0124 - mae: 0.0893 - val_loss: 0.0107 - val_mae: 0.0808\n",
            "Epoch 143/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0122 - mae: 0.0889 - val_loss: 0.0086 - val_mae: 0.0712\n",
            "Epoch 144/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0122 - mae: 0.0891 - val_loss: 0.0133 - val_mae: 0.0910\n",
            "Epoch 145/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0120 - mae: 0.0877 - val_loss: 0.0099 - val_mae: 0.0778\n",
            "Epoch 146/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0125 - mae: 0.0905 - val_loss: 0.0083 - val_mae: 0.0700\n",
            "Epoch 147/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0125 - mae: 0.0904 - val_loss: 0.0109 - val_mae: 0.0805\n",
            "Epoch 148/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0126 - mae: 0.0904 - val_loss: 0.0086 - val_mae: 0.0717\n",
            "Epoch 149/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0119 - mae: 0.0882 - val_loss: 0.0122 - val_mae: 0.0881\n",
            "Epoch 150/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0125 - mae: 0.0899 - val_loss: 0.0104 - val_mae: 0.0785\n",
            "Epoch 151/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0123 - mae: 0.0881 - val_loss: 0.0121 - val_mae: 0.0881\n",
            "Epoch 152/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0126 - mae: 0.0901 - val_loss: 0.0102 - val_mae: 0.0777\n",
            "Epoch 153/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0125 - mae: 0.0898 - val_loss: 0.0125 - val_mae: 0.0890\n",
            "Epoch 154/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0116 - mae: 0.0878 - val_loss: 0.0102 - val_mae: 0.0780\n",
            "Epoch 155/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0126 - mae: 0.0898 - val_loss: 0.0104 - val_mae: 0.0808\n",
            "Epoch 156/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0121 - mae: 0.0895 - val_loss: 0.0137 - val_mae: 0.0930\n",
            "Epoch 157/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0124 - mae: 0.0879 - val_loss: 0.0090 - val_mae: 0.0736\n",
            "Epoch 158/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0120 - mae: 0.0883 - val_loss: 0.0109 - val_mae: 0.0820\n",
            "Epoch 159/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0123 - mae: 0.0890 - val_loss: 0.0101 - val_mae: 0.0792\n",
            "Epoch 160/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0122 - mae: 0.0891 - val_loss: 0.0089 - val_mae: 0.0740\n",
            "Epoch 161/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0124 - mae: 0.0895 - val_loss: 0.0142 - val_mae: 0.0957\n",
            "Epoch 162/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0118 - mae: 0.0866 - val_loss: 0.0088 - val_mae: 0.0735\n",
            "Epoch 163/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0127 - mae: 0.0901 - val_loss: 0.0125 - val_mae: 0.0891\n",
            "Epoch 164/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0122 - mae: 0.0890 - val_loss: 0.0113 - val_mae: 0.0845\n",
            "Epoch 165/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0123 - mae: 0.0903 - val_loss: 0.0129 - val_mae: 0.0892\n",
            "Epoch 166/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0125 - mae: 0.0897 - val_loss: 0.0101 - val_mae: 0.0791\n",
            "Epoch 167/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0121 - mae: 0.0888 - val_loss: 0.0088 - val_mae: 0.0728\n",
            "Epoch 168/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0117 - mae: 0.0878 - val_loss: 0.0110 - val_mae: 0.0840\n",
            "Epoch 169/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0126 - mae: 0.0898 - val_loss: 0.0151 - val_mae: 0.0980\n",
            "Epoch 170/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0120 - mae: 0.0869 - val_loss: 0.0111 - val_mae: 0.0823\n",
            "Epoch 171/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0122 - mae: 0.0888 - val_loss: 0.0142 - val_mae: 0.0925\n",
            "Epoch 172/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0121 - mae: 0.0875 - val_loss: 0.0106 - val_mae: 0.0803\n",
            "Epoch 173/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0121 - mae: 0.0886 - val_loss: 0.0102 - val_mae: 0.0793\n",
            "Epoch 174/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0115 - mae: 0.0866 - val_loss: 0.0089 - val_mae: 0.0729\n",
            "Epoch 175/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0117 - mae: 0.0872 - val_loss: 0.0091 - val_mae: 0.0730\n",
            "Epoch 176/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0123 - mae: 0.0888 - val_loss: 0.0110 - val_mae: 0.0829\n",
            "Epoch 177/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0124 - mae: 0.0895 - val_loss: 0.0098 - val_mae: 0.0765\n",
            "Epoch 178/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0121 - mae: 0.0887 - val_loss: 0.0099 - val_mae: 0.0781\n",
            "Epoch 179/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0121 - mae: 0.0883 - val_loss: 0.0103 - val_mae: 0.0811\n",
            "Epoch 180/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0124 - mae: 0.0892 - val_loss: 0.0084 - val_mae: 0.0710\n",
            "Epoch 181/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0124 - mae: 0.0895 - val_loss: 0.0093 - val_mae: 0.0736\n",
            "Epoch 182/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0116 - mae: 0.0868 - val_loss: 0.0100 - val_mae: 0.0774\n",
            "Epoch 183/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0124 - mae: 0.0888 - val_loss: 0.0106 - val_mae: 0.0806\n",
            "Epoch 184/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0119 - mae: 0.0880 - val_loss: 0.0091 - val_mae: 0.0735\n",
            "Epoch 185/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0124 - mae: 0.0881 - val_loss: 0.0090 - val_mae: 0.0737\n",
            "Epoch 186/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0119 - mae: 0.0878 - val_loss: 0.0092 - val_mae: 0.0734\n",
            "Epoch 187/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0123 - mae: 0.0895 - val_loss: 0.0126 - val_mae: 0.0892\n",
            "Epoch 188/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0119 - mae: 0.0881 - val_loss: 0.0102 - val_mae: 0.0788\n",
            "Epoch 189/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0123 - mae: 0.0878 - val_loss: 0.0104 - val_mae: 0.0793\n",
            "Epoch 190/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0120 - mae: 0.0885 - val_loss: 0.0094 - val_mae: 0.0751\n",
            "Epoch 191/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0124 - mae: 0.0891 - val_loss: 0.0121 - val_mae: 0.0873\n",
            "Epoch 192/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0119 - mae: 0.0885 - val_loss: 0.0084 - val_mae: 0.0705\n",
            "Epoch 193/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0126 - mae: 0.0906 - val_loss: 0.0091 - val_mae: 0.0738\n",
            "Epoch 194/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0114 - mae: 0.0854 - val_loss: 0.0111 - val_mae: 0.0848\n",
            "Epoch 195/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0122 - mae: 0.0889 - val_loss: 0.0112 - val_mae: 0.0835\n",
            "Epoch 196/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0116 - mae: 0.0871 - val_loss: 0.0092 - val_mae: 0.0739\n",
            "Epoch 197/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0122 - mae: 0.0892 - val_loss: 0.0160 - val_mae: 0.0983\n",
            "Epoch 198/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0120 - mae: 0.0882 - val_loss: 0.0092 - val_mae: 0.0741\n",
            "Epoch 199/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0121 - mae: 0.0886 - val_loss: 0.0109 - val_mae: 0.0812\n",
            "Epoch 200/200\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0125 - mae: 0.0899 - val_loss: 0.0089 - val_mae: 0.0725\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "INFO:tensorflow:Assets written to: /tmp/tmp3whgtv_x/assets\n",
            "\n",
            "#ifdef __has_attribute\n",
            "#define HAVE_ATTRIBUTE(x) __has_attribute(x)\n",
            "#else\n",
            "#define HAVE_ATTRIBUTE(x) 0\n",
            "#endif\n",
            "#if HAVE_ATTRIBUTE(aligned) || (defined(__GNUC__) && !defined(__clang__))\n",
            "#define DATA_ALIGN_ATTRIBUTE __attribute__((aligned(4)))\n",
            "#else\n",
            "#define DATA_ALIGN_ATTRIBUTE\n",
            "#endif\n",
            "\n",
            "const unsigned char model_data[] DATA_ALIGN_ATTRIBUTE = {\n",
            "\t0x20, 0x00, 0x00, 0x00, 0x54, 0x46, 0x4c, 0x33, 0x00, 0x00, 0x00, 0x00, \n",
            "\t0x00, 0x00, 0x12, 0x00, 0x1c, 0x00, 0x04, 0x00, 0x08, 0x00, 0x0c, 0x00, \n",
            "\t0x10, 0x00, 0x14, 0x00, 0x00, 0x00, 0x18, 0x00, 0x12, 0x00, 0x00, 0x00, \n",
            "\t0x03, 0x00, 0x00, 0x00, 0x28, 0x0b, 0x00, 0x00, 0x68, 0x06, 0x00, 0x00, \n",
            "\t0x50, 0x06, 0x00, 0x00, 0x3c, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, \n",
            "\t0x01, 0x00, 0x00, 0x00, 0x0c, 0x00, 0x00, 0x00, 0x08, 0x00, 0x0c, 0x00, \n",
            "\t0x04, 0x00, 0x08, 0x00, 0x08, 0x00, 0x00, 0x00, 0x08, 0x00, 0x00, 0x00, \n",
            "\t0x0b, 0x00, 0x00, 0x00, 0x13, 0x00, 0x00, 0x00, 0x6d, 0x69, 0x6e, 0x5f, \n",
            "\t0x72, 0x75, 0x6e, 0x74, 0x69, 0x6d, 0x65, 0x5f, 0x76, 0x65, 0x72, 0x73, \n",
            "\t0x69, 0x6f, 0x6e, 0x00, 0x0c, 0x00, 0x00, 0x00, 0x08, 0x06, 0x00, 0x00, \n",
            "\t0xf4, 0x05, 0x00, 0x00, 0x98, 0x05, 0x00, 0x00, 0x34, 0x05, 0x00, 0x00, \n",
            "\t0x20, 0x05, 0x00, 0x00, 0xcc, 0x04, 0x00, 0x00, 0xb8, 0x00, 0x00, 0x00, \n",
            "\t0x64, 0x00, 0x00, 0x00, 0x58, 0x00, 0x00, 0x00, 0x44, 0x00, 0x00, 0x00, \n",
            "\t0x30, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x96, 0xfa, 0xff, 0xff, \n",
            "\t0x04, 0x00, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00, 0x31, 0x2e, 0x35, 0x2e, \n",
            "\t0x30, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, \n",
            "\t0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, \n",
            "\t0xa4, 0xf5, 0xff, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, \n",
            "\t0x00, 0x00, 0x00, 0x00, 0xb4, 0xf5, 0xff, 0xff, 0x00, 0x00, 0x00, 0x00, \n",
            "\t0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0xc4, 0xf5, 0xff, 0xff, \n",
            "\t0x00, 0x00, 0x00, 0x00, 0xe6, 0xfa, 0xff, 0xff, 0x04, 0x00, 0x00, 0x00, \n",
            "\t0x40, 0x00, 0x00, 0x00, 0xba, 0x62, 0x2c, 0x3f, 0xea, 0xc8, 0xbc, 0xbd, \n",
            "\t0x60, 0xf1, 0xd9, 0xbd, 0xb4, 0xbb, 0x97, 0xbf, 0x58, 0x60, 0xa9, 0x3e, \n",
            "\t0xf7, 0xa9, 0x87, 0x3f, 0x54, 0xce, 0x90, 0xbe, 0x5c, 0x93, 0x63, 0x3d, \n",
            "\t0xd6, 0xf3, 0x09, 0xbf, 0x48, 0x84, 0x8d, 0x3f, 0x20, 0x22, 0x81, 0x3e, \n",
            "\t0xd7, 0xdf, 0xcf, 0x3f, 0x38, 0x0e, 0x9e, 0x3e, 0x41, 0x91, 0x92, 0x3f, \n",
            "\t0x09, 0xc3, 0x95, 0x3f, 0x44, 0x72, 0xe8, 0xbe, 0x00, 0x00, 0x00, 0x00, \n",
            "\t0x36, 0xfb, 0xff, 0xff, 0x04, 0x00, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, \n",
            "\t0x00, 0x27, 0x63, 0xbc, 0x55, 0x11, 0xa2, 0x3e, 0xcd, 0x6d, 0x9d, 0x3e, \n",
            "\t0x1b, 0x9f, 0x7d, 0x3c, 0x5f, 0xe7, 0xfe, 0xbe, 0x31, 0x06, 0x89, 0xbc, \n",
            "\t0xca, 0x7f, 0x5e, 0x3e, 0xa3, 0xbf, 0xa1, 0x3e, 0x14, 0x85, 0xa5, 0x3e, \n",
            "\t0x8b, 0xd7, 0xc4, 0x3e, 0xf6, 0x3d, 0x46, 0x3e, 0x75, 0x24, 0xc8, 0x3e, \n",
            "\t0x0e, 0xf5, 0x8a, 0xbe, 0x34, 0x3a, 0x9d, 0x3d, 0xf4, 0x62, 0x9f, 0x3d, \n",
            "\t0x94, 0x20, 0xfd, 0x3d, 0x00, 0xb1, 0x9b, 0x3b, 0xf9, 0xa0, 0x6e, 0xbd, \n",
            "\t0xe2, 0x37, 0x3d, 0xbd, 0xad, 0xef, 0xc2, 0xbd, 0x84, 0xf9, 0x5b, 0xbe, \n",
            "\t0xd1, 0x75, 0x04, 0x3f, 0x19, 0xd9, 0x81, 0x3e, 0x74, 0xc3, 0x02, 0xbe, \n",
            "\t0x42, 0x73, 0x85, 0x3e, 0x40, 0x4a, 0xe2, 0xbb, 0x51, 0x94, 0xd9, 0xbe, \n",
            "\t0x2c, 0x1e, 0xba, 0xbd, 0x13, 0x41, 0x8c, 0x3e, 0xba, 0xe9, 0x35, 0xbe, \n",
            "\t0x1b, 0x6b, 0xda, 0xbe, 0x57, 0xf1, 0xa9, 0xbe, 0x98, 0x89, 0x21, 0x3d, \n",
            "\t0x12, 0x57, 0x53, 0x3e, 0xd0, 0xfe, 0xe6, 0xbc, 0x97, 0x22, 0x50, 0xbe, \n",
            "\t0xc8, 0xb1, 0x98, 0xbd, 0xe9, 0x78, 0x4b, 0xbe, 0x56, 0xf3, 0xad, 0xbe, \n",
            "\t0xbe, 0xd2, 0xbd, 0xbe, 0x20, 0xba, 0xbb, 0xbd, 0xa1, 0xbd, 0xb0, 0x3e, \n",
            "\t0x8a, 0x85, 0x86, 0xbe, 0xf9, 0xae, 0x83, 0x3e, 0x8b, 0xa0, 0xb1, 0x3e, \n",
            "\t0x62, 0x40, 0x7e, 0x3e, 0x65, 0x22, 0xbc, 0x3e, 0xf0, 0xd2, 0x75, 0xbd, \n",
            "\t0x78, 0xa7, 0xf0, 0xbd, 0x35, 0xad, 0xe2, 0x3e, 0x74, 0x8e, 0x96, 0xbd, \n",
            "\t0x45, 0xed, 0xdb, 0x3e, 0x61, 0x79, 0x35, 0x3e, 0xc0, 0x0d, 0xa2, 0xbf, \n",
            "\t0x14, 0x6c, 0xb8, 0x3d, 0x04, 0x3a, 0x9d, 0xbe, 0x33, 0xd6, 0x2f, 0xbf, \n",
            "\t0x7d, 0x1f, 0x9e, 0xbe, 0xd8, 0x71, 0x80, 0x3d, 0x20, 0x47, 0x26, 0xbe, \n",
            "\t0x84, 0x4e, 0x46, 0x3e, 0xa8, 0x2b, 0x19, 0xbe, 0x8c, 0xe2, 0xb8, 0x3d, \n",
            "\t0xc0, 0x5d, 0xae, 0x3c, 0x68, 0xaa, 0x84, 0xbe, 0x17, 0xdf, 0xd6, 0x3e, \n",
            "\t0x0f, 0x42, 0x54, 0xbe, 0x13, 0x87, 0x93, 0x3e, 0xe3, 0x24, 0xa8, 0xbe, \n",
            "\t0x2a, 0xec, 0xbf, 0xbe, 0x6e, 0x8f, 0x30, 0x3e, 0xc4, 0xaa, 0x90, 0x3d, \n",
            "\t0xc3, 0xdc, 0x99, 0xbe, 0xa7, 0x67, 0xbd, 0xbe, 0x50, 0x5a, 0xfa, 0xbd, \n",
            "\t0x32, 0xe4, 0x8e, 0xbe, 0x55, 0x15, 0x8f, 0x3e, 0x33, 0x3c, 0xcc, 0x3e, \n",
            "\t0x59, 0xca, 0xa5, 0xbe, 0x07, 0x2f, 0xdd, 0x3e, 0x42, 0x4c, 0x4a, 0x3e, \n",
            "\t0x92, 0xbd, 0x1f, 0xbf, 0x85, 0x61, 0x2c, 0xbf, 0x4b, 0x66, 0x8b, 0x3e, \n",
            "\t0x2c, 0x14, 0xe8, 0x3e, 0xf6, 0xdd, 0x8a, 0x3e, 0xf0, 0xff, 0xda, 0xbd, \n",
            "\t0x58, 0x15, 0xc2, 0xbd, 0x27, 0x30, 0x12, 0xbe, 0xf4, 0xe8, 0x99, 0x3d, \n",
            "\t0x7d, 0x4d, 0x90, 0x3e, 0x4d, 0x8f, 0x5a, 0xbe, 0xe8, 0xc0, 0x32, 0xbf, \n",
            "\t0xfd, 0x10, 0xd8, 0xbe, 0x72, 0x72, 0x1a, 0xbe, 0xd3, 0xbc, 0xcd, 0x3e, \n",
            "\t0x66, 0x08, 0x28, 0x3e, 0xcc, 0x7a, 0x2b, 0xbe, 0x1c, 0x13, 0x85, 0xbe, \n",
            "\t0xf2, 0x46, 0xbd, 0xbe, 0x11, 0x17, 0x83, 0xbe, 0x72, 0x6e, 0x1d, 0x3e, \n",
            "\t0xba, 0xa2, 0x2e, 0x3e, 0xd2, 0xa7, 0x0c, 0xbe, 0xfc, 0xfd, 0xf4, 0x3d, \n",
            "\t0xad, 0x00, 0x4a, 0xbe, 0x20, 0xb9, 0xc6, 0x3c, 0xd8, 0x31, 0x4e, 0xbe, \n",
            "\t0x80, 0x35, 0xe5, 0xbb, 0x41, 0xb0, 0x58, 0xbe, 0x4c, 0xaf, 0xde, 0x3d, \n",
            "\t0x00, 0xe9, 0x46, 0x3d, 0xbf, 0x93, 0xa7, 0xbe, 0x20, 0x53, 0x57, 0x3e, \n",
            "\t0x47, 0x33, 0xa3, 0x3c, 0xc7, 0x67, 0x85, 0x3e, 0x91, 0x6f, 0x09, 0xbd, \n",
            "\t0x66, 0x91, 0x96, 0x3e, 0xbf, 0x7a, 0xab, 0x3e, 0x2d, 0x0d, 0xcc, 0x3e, \n",
            "\t0x3d, 0xfe, 0x98, 0x3d, 0x0a, 0x6a, 0xb6, 0xbe, 0xd6, 0x98, 0x59, 0x3e, \n",
            "\t0x1c, 0xe8, 0x99, 0x3d, 0x5b, 0x28, 0x3d, 0x3e, 0x90, 0x40, 0x47, 0x3d, \n",
            "\t0x0c, 0x30, 0xe1, 0x3d, 0x1e, 0x81, 0x40, 0x3e, 0xb7, 0x95, 0x97, 0x3e, \n",
            "\t0x04, 0xd2, 0xde, 0x3e, 0xdf, 0x17, 0x99, 0xbe, 0xc9, 0x62, 0x96, 0x3e, \n",
            "\t0xe4, 0x60, 0xa6, 0x3c, 0x48, 0x91, 0xa7, 0x3e, 0x1c, 0xdc, 0xce, 0x3d, \n",
            "\t0x9a, 0x58, 0xc7, 0xbe, 0xbd, 0xc8, 0xe8, 0x3c, 0x49, 0xae, 0xa0, 0xbe, \n",
            "\t0x54, 0xe1, 0xe1, 0xbd, 0x0c, 0x90, 0x99, 0xbe, 0x21, 0x97, 0x90, 0xbe, \n",
            "\t0xac, 0xb5, 0x16, 0xbe, 0x02, 0x3f, 0x95, 0xbe, 0xd0, 0xe3, 0x3a, 0xbe, \n",
            "\t0x99, 0x29, 0xca, 0xbe, 0x05, 0x44, 0x08, 0x3c, 0x63, 0x2f, 0x63, 0xbf, \n",
            "\t0x5b, 0x22, 0xab, 0x3e, 0xf6, 0x85, 0x0a, 0x3f, 0xc1, 0x2d, 0x86, 0xbe, \n",
            "\t0x58, 0xd3, 0x32, 0xbe, 0x54, 0xdd, 0xdb, 0x3d, 0x78, 0x95, 0xd4, 0xbd, \n",
            "\t0xf4, 0xbb, 0x9d, 0x3d, 0x92, 0xe8, 0x55, 0x3e, 0x70, 0x01, 0x52, 0xbd, \n",
            "\t0x69, 0x79, 0x3e, 0xbf, 0x80, 0xb5, 0xd2, 0xbc, 0x7b, 0x1d, 0x59, 0xbe, \n",
            "\t0x86, 0xf9, 0xa8, 0xbe, 0xbc, 0x56, 0x89, 0x3d, 0x52, 0x44, 0x4a, 0x3e, \n",
            "\t0x20, 0xef, 0xdf, 0xbe, 0x16, 0x9d, 0x98, 0xbe, 0x9a, 0x93, 0xee, 0xbe, \n",
            "\t0x93, 0xed, 0x83, 0x3e, 0xc8, 0xd4, 0x7c, 0xbd, 0x2a, 0xc3, 0xa5, 0xbe, \n",
            "\t0xbb, 0x33, 0x10, 0xbe, 0xce, 0xcd, 0x48, 0x3e, 0x43, 0xf7, 0xd9, 0xbe, \n",
            "\t0x17, 0xc6, 0xda, 0x3e, 0x5b, 0x5d, 0xa7, 0x3e, 0xc9, 0x72, 0x86, 0x3e, \n",
            "\t0x8d, 0xbf, 0xbd, 0x3e, 0x36, 0xf9, 0x24, 0x3e, 0x3d, 0x75, 0xa6, 0xbe, \n",
            "\t0x09, 0x84, 0x20, 0xbd, 0xdd, 0x68, 0x18, 0x3f, 0x83, 0xea, 0x0e, 0xbf, \n",
            "\t0x26, 0x81, 0xd0, 0xba, 0x62, 0xe1, 0x5c, 0xbb, 0xa0, 0x8d, 0x82, 0xbc, \n",
            "\t0xd9, 0x09, 0xb7, 0x3e, 0xf6, 0x8d, 0xa0, 0x3e, 0xe7, 0x08, 0x47, 0xbe, \n",
            "\t0x54, 0x7f, 0x75, 0xbe, 0x5d, 0x27, 0x94, 0x3e, 0xeb, 0xfa, 0x2e, 0x3f, \n",
            "\t0xe0, 0x13, 0x99, 0xbc, 0xa0, 0x08, 0x6a, 0x3d, 0x70, 0xfb, 0x8e, 0xbc, \n",
            "\t0xb0, 0x8a, 0xe7, 0xbc, 0xd8, 0x03, 0x41, 0xbd, 0x46, 0x04, 0x4e, 0x3e, \n",
            "\t0x60, 0x73, 0xf4, 0xbd, 0x18, 0xdf, 0x28, 0xbe, 0x72, 0xb0, 0x5b, 0x3e, \n",
            "\t0x67, 0xff, 0x9f, 0x3e, 0x5a, 0x24, 0xbe, 0xbe, 0xf8, 0xfc, 0x9a, 0xbe, \n",
            "\t0xb0, 0x07, 0x79, 0x3d, 0x81, 0xaf, 0xb7, 0x3e, 0xd0, 0x8d, 0x6c, 0xbd, \n",
            "\t0x3a, 0xe9, 0x51, 0xbe, 0xf2, 0xe5, 0x3b, 0x3e, 0xfa, 0x6c, 0x06, 0x3e, \n",
            "\t0xef, 0xc5, 0x58, 0xbe, 0x2a, 0xc1, 0xb8, 0xbe, 0xc7, 0xbe, 0xb5, 0x3e, \n",
            "\t0x1e, 0x8e, 0xb0, 0x3e, 0x90, 0xf7, 0x84, 0x3d, 0xb2, 0xe5, 0x06, 0xbf, \n",
            "\t0xfe, 0x81, 0xcf, 0xbd, 0xb4, 0x41, 0x90, 0xbd, 0x7a, 0x88, 0x75, 0x3e, \n",
            "\t0x35, 0x8a, 0x1c, 0x3e, 0x98, 0xb5, 0x6e, 0xbe, 0xe5, 0x4c, 0x85, 0x3e, \n",
            "\t0x18, 0x79, 0xbd, 0xbe, 0xb1, 0x2a, 0xf5, 0xbc, 0x58, 0x44, 0x9b, 0xbe, \n",
            "\t0x1c, 0xd8, 0xc9, 0xbe, 0x90, 0x16, 0xd3, 0xbe, 0x4f, 0x0f, 0xb3, 0x3e, \n",
            "\t0xa0, 0xd0, 0x9d, 0xbe, 0xe6, 0x6b, 0xcf, 0x3e, 0x39, 0x3d, 0x2a, 0x3e, \n",
            "\t0x05, 0x04, 0x9c, 0xbe, 0xba, 0x9f, 0x1f, 0xbe, 0xe4, 0x60, 0xe1, 0x3d, \n",
            "\t0xf0, 0xac, 0xb1, 0xbc, 0xae, 0x6f, 0x57, 0x3e, 0x64, 0x6c, 0xc5, 0x3d, \n",
            "\t0x9a, 0xa6, 0x3d, 0x3e, 0x0e, 0x46, 0x1e, 0xbe, 0xaa, 0xc7, 0x05, 0x3f, \n",
            "\t0x10, 0x9a, 0xeb, 0xbd, 0x60, 0x6b, 0x9e, 0xbd, 0xf0, 0x1e, 0xcc, 0xbd, \n",
            "\t0x1a, 0x20, 0x16, 0x3e, 0x4e, 0x10, 0x76, 0x3e, 0x21, 0xdc, 0x9a, 0x3e, \n",
            "\t0x0a, 0x19, 0xb1, 0xbe, 0x94, 0x3d, 0xe9, 0xbd, 0x82, 0x80, 0x8b, 0xbe, \n",
            "\t0x3b, 0x79, 0xa6, 0xbe, 0x8d, 0xf9, 0xba, 0x3e, 0xce, 0xa8, 0x32, 0xbe, \n",
            "\t0x99, 0x16, 0xda, 0x3e, 0x8c, 0x27, 0xbb, 0x3d, 0x1a, 0x00, 0x9f, 0xbe, \n",
            "\t0xf0, 0x6f, 0x05, 0xbd, 0x70, 0xbf, 0x7b, 0xbd, 0x1a, 0xa1, 0x41, 0x3e, \n",
            "\t0x0c, 0x73, 0xcf, 0x3d, 0x00, 0x00, 0x00, 0x00, 0x46, 0xff, 0xff, 0xff, \n",
            "\t0x04, 0x00, 0x00, 0x00, 0x40, 0x00, 0x00, 0x00, 0x0a, 0xee, 0xca, 0xbe, \n",
            "\t0xb3, 0x63, 0xa4, 0x3e, 0x61, 0x67, 0x5d, 0x3e, 0x48, 0x7f, 0x55, 0x3e, \n",
            "\t0xe0, 0xa5, 0x0d, 0x3e, 0x7a, 0x5a, 0xd2, 0x3e, 0x47, 0xfc, 0xf4, 0xbe, \n",
            "\t0x70, 0xc8, 0x0a, 0xbf, 0x6b, 0x7c, 0xe5, 0x3e, 0xe0, 0x90, 0xe5, 0xbd, \n",
            "\t0x64, 0xe2, 0x57, 0xbe, 0x46, 0x05, 0x0b, 0xbf, 0x52, 0x1f, 0x8c, 0x3e, \n",
            "\t0x71, 0xc7, 0xe9, 0xbe, 0x6b, 0xb6, 0x9f, 0xbe, 0x04, 0x5a, 0x75, 0xbe, \n",
            "\t0x00, 0x00, 0x00, 0x00, 0x96, 0xff, 0xff, 0xff, 0x04, 0x00, 0x00, 0x00, \n",
            "\t0x04, 0x00, 0x00, 0x00, 0xe0, 0xfa, 0x8c, 0xbe, 0xa6, 0xff, 0xff, 0xff, \n",
            "\t0x04, 0x00, 0x00, 0x00, 0x40, 0x00, 0x00, 0x00, 0xf1, 0x33, 0x4e, 0xbe, \n",
            "\t0x3a, 0x5b, 0x2a, 0x3d, 0x00, 0x00, 0x00, 0x00, 0x81, 0x2d, 0x04, 0x3f, \n",
            "\t0x84, 0xcd, 0xeb, 0xbc, 0xa8, 0xda, 0x52, 0x3e, 0x00, 0x00, 0x00, 0x00, \n",
            "\t0x09, 0x64, 0x2f, 0xbe, 0x43, 0x9d, 0xac, 0x3c, 0x7b, 0x85, 0x8d, 0x3e, \n",
            "\t0xc6, 0xfe, 0x26, 0xbd, 0x73, 0xf8, 0x01, 0xbf, 0x00, 0x00, 0x00, 0x00, \n",
            "\t0x61, 0xe6, 0x8f, 0xbe, 0xef, 0xf7, 0xa1, 0xbe, 0x00, 0x00, 0x00, 0x00, \n",
            "\t0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, \n",
            "\t0x00, 0x00, 0x06, 0x00, 0x08, 0x00, 0x04, 0x00, 0x06, 0x00, 0x00, 0x00, \n",
            "\t0x04, 0x00, 0x00, 0x00, 0x40, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, \n",
            "\t0xf5, 0xcf, 0x86, 0xbe, 0xe2, 0x73, 0x0e, 0xbf, 0xe4, 0x40, 0x2c, 0x3f, \n",
            "\t0xc1, 0xe2, 0xfe, 0x3e, 0xae, 0x16, 0x01, 0x3d, 0x00, 0x00, 0x00, 0x00, \n",
            "\t0x00, 0x00, 0x00, 0x00, 0x4c, 0xf3, 0x25, 0xbe, 0x00, 0x00, 0x00, 0x00, \n",
            "\t0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0xfa, 0x24, 0x0f, 0xbf, \n",
            "\t0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, \n",
            "\t0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, \n",
            "\t0x44, 0xfb, 0xff, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, \n",
            "\t0x00, 0x00, 0x00, 0x00, 0x54, 0xfb, 0xff, 0xff, 0x0f, 0x00, 0x00, 0x00, \n",
            "\t0x4d, 0x4c, 0x49, 0x52, 0x20, 0x43, 0x6f, 0x6e, 0x76, 0x65, 0x72, 0x74, \n",
            "\t0x65, 0x64, 0x2e, 0x00, 0x01, 0x00, 0x00, 0x00, 0x14, 0x00, 0x00, 0x00, \n",
            "\t0x00, 0x00, 0x0e, 0x00, 0x18, 0x00, 0x04, 0x00, 0x08, 0x00, 0x0c, 0x00, \n",
            "\t0x10, 0x00, 0x14, 0x00, 0x0e, 0x00, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, \n",
            "\t0xf4, 0x00, 0x00, 0x00, 0xe8, 0x00, 0x00, 0x00, 0x14, 0x00, 0x00, 0x00, \n",
            "\t0x04, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x6d, 0x61, 0x69, 0x6e, \n",
            "\t0x00, 0x00, 0x00, 0x00, 0x03, 0x00, 0x00, 0x00, 0x90, 0x00, 0x00, 0x00, \n",
            "\t0x48, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0xce, 0xff, 0xff, 0xff, \n",
            "\t0x00, 0x00, 0x00, 0x08, 0x18, 0x00, 0x00, 0x00, 0x0c, 0x00, 0x00, 0x00, \n",
            "\t0x04, 0x00, 0x00, 0x00, 0xcc, 0xfb, 0xff, 0xff, 0x01, 0x00, 0x00, 0x00, \n",
            "\t0x09, 0x00, 0x00, 0x00, 0x03, 0x00, 0x00, 0x00, 0x08, 0x00, 0x00, 0x00, \n",
            "\t0x06, 0x00, 0x00, 0x00, 0x03, 0x00, 0x00, 0x00, 0x00, 0x00, 0x0e, 0x00, \n",
            "\t0x14, 0x00, 0x00, 0x00, 0x08, 0x00, 0x0c, 0x00, 0x07, 0x00, 0x10, 0x00, \n",
            "\t0x0e, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x08, 0x1c, 0x00, 0x00, 0x00, \n",
            "\t0x10, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0xba, 0xff, 0xff, 0xff, \n",
            "\t0x00, 0x00, 0x00, 0x01, 0x01, 0x00, 0x00, 0x00, 0x08, 0x00, 0x00, 0x00, \n",
            "\t0x03, 0x00, 0x00, 0x00, 0x07, 0x00, 0x00, 0x00, 0x05, 0x00, 0x00, 0x00, \n",
            "\t0x02, 0x00, 0x00, 0x00, 0x00, 0x00, 0x0e, 0x00, 0x16, 0x00, 0x00, 0x00, \n",
            "\t0x08, 0x00, 0x0c, 0x00, 0x07, 0x00, 0x10, 0x00, 0x0e, 0x00, 0x00, 0x00, \n",
            "\t0x00, 0x00, 0x00, 0x08, 0x24, 0x00, 0x00, 0x00, 0x18, 0x00, 0x00, 0x00, \n",
            "\t0x0c, 0x00, 0x00, 0x00, 0x00, 0x00, 0x06, 0x00, 0x08, 0x00, 0x07, 0x00, \n",
            "\t0x06, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x01, 0x01, 0x00, 0x00, 0x00, \n",
            "\t0x07, 0x00, 0x00, 0x00, 0x03, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, \n",
            "\t0x04, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, \n",
            "\t0x09, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, \n",
            "\t0x0a, 0x00, 0x00, 0x00, 0x50, 0x03, 0x00, 0x00, 0xdc, 0x02, 0x00, 0x00, \n",
            "\t0x6c, 0x02, 0x00, 0x00, 0x0c, 0x02, 0x00, 0x00, 0xc4, 0x01, 0x00, 0x00, \n",
            "\t0x7c, 0x01, 0x00, 0x00, 0x34, 0x01, 0x00, 0x00, 0xc0, 0x00, 0x00, 0x00, \n",
            "\t0x4c, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0xec, 0xfc, 0xff, 0xff, \n",
            "\t0x34, 0x00, 0x00, 0x00, 0x0a, 0x00, 0x00, 0x00, 0x1c, 0x00, 0x00, 0x00, \n",
            "\t0x14, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, \n",
            "\t0xff, 0xff, 0xff, 0xff, 0x01, 0x00, 0x00, 0x00, 0xd8, 0xfc, 0xff, 0xff, \n",
            "\t0x08, 0x00, 0x00, 0x00, 0x49, 0x64, 0x65, 0x6e, 0x74, 0x69, 0x74, 0x79, \n",
            "\t0x00, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, \n",
            "\t0x01, 0x00, 0x00, 0x00, 0x30, 0xfd, 0xff, 0xff, 0x60, 0x00, 0x00, 0x00, \n",
            "\t0x09, 0x00, 0x00, 0x00, 0x1c, 0x00, 0x00, 0x00, 0x14, 0x00, 0x00, 0x00, \n",
            "\t0x04, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 0xff, 0xff, 0xff, 0xff, \n",
            "\t0x10, 0x00, 0x00, 0x00, 0x1c, 0xfd, 0xff, 0xff, 0x36, 0x00, 0x00, 0x00, \n",
            "\t0x73, 0x65, 0x71, 0x75, 0x65, 0x6e, 0x74, 0x69, 0x61, 0x6c, 0x5f, 0x31, \n",
            "\t0x2f, 0x64, 0x65, 0x6e, 0x73, 0x65, 0x5f, 0x34, 0x2f, 0x52, 0x65, 0x6c, \n",
            "\t0x75, 0x3b, 0x73, 0x65, 0x71, 0x75, 0x65, 0x6e, 0x74, 0x69, 0x61, 0x6c, \n",
            "\t0x5f, 0x31, 0x2f, 0x64, 0x65, 0x6e, 0x73, 0x65, 0x5f, 0x34, 0x2f, 0x42, \n",
            "\t0x69, 0x61, 0x73, 0x41, 0x64, 0x64, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, \n",
            "\t0x01, 0x00, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00, 0xa0, 0xfd, 0xff, 0xff, \n",
            "\t0x60, 0x00, 0x00, 0x00, 0x08, 0x00, 0x00, 0x00, 0x1c, 0x00, 0x00, 0x00, \n",
            "\t0x14, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, \n",
            "\t0xff, 0xff, 0xff, 0xff, 0x10, 0x00, 0x00, 0x00, 0x8c, 0xfd, 0xff, 0xff, \n",
            "\t0x36, 0x00, 0x00, 0x00, 0x73, 0x65, 0x71, 0x75, 0x65, 0x6e, 0x74, 0x69, \n",
            "\t0x61, 0x6c, 0x5f, 0x31, 0x2f, 0x64, 0x65, 0x6e, 0x73, 0x65, 0x5f, 0x33, \n",
            "\t0x2f, 0x52, 0x65, 0x6c, 0x75, 0x3b, 0x73, 0x65, 0x71, 0x75, 0x65, 0x6e, \n",
            "\t0x74, 0x69, 0x61, 0x6c, 0x5f, 0x31, 0x2f, 0x64, 0x65, 0x6e, 0x73, 0x65, \n",
            "\t0x5f, 0x33, 0x2f, 0x42, 0x69, 0x61, 0x73, 0x41, 0x64, 0x64, 0x00, 0x00, \n",
            "\t0x02, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00, \n",
            "\t0x7a, 0xfe, 0xff, 0xff, 0x34, 0x00, 0x00, 0x00, 0x07, 0x00, 0x00, 0x00, \n",
            "\t0x0c, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0xec, 0xfd, 0xff, 0xff, \n",
            "\t0x1b, 0x00, 0x00, 0x00, 0x73, 0x65, 0x71, 0x75, 0x65, 0x6e, 0x74, 0x69, \n",
            "\t0x61, 0x6c, 0x5f, 0x31, 0x2f, 0x64, 0x65, 0x6e, 0x73, 0x65, 0x5f, 0x35, \n",
            "\t0x2f, 0x4d, 0x61, 0x74, 0x4d, 0x75, 0x6c, 0x00, 0x02, 0x00, 0x00, 0x00, \n",
            "\t0x01, 0x00, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00, 0xbe, 0xfe, 0xff, 0xff, \n",
            "\t0x34, 0x00, 0x00, 0x00, 0x06, 0x00, 0x00, 0x00, 0x0c, 0x00, 0x00, 0x00, \n",
            "\t0x04, 0x00, 0x00, 0x00, 0x30, 0xfe, 0xff, 0xff, 0x1b, 0x00, 0x00, 0x00, \n",
            "\t0x73, 0x65, 0x71, 0x75, 0x65, 0x6e, 0x74, 0x69, 0x61, 0x6c, 0x5f, 0x31, \n",
            "\t0x2f, 0x64, 0x65, 0x6e, 0x73, 0x65, 0x5f, 0x34, 0x2f, 0x4d, 0x61, 0x74, \n",
            "\t0x4d, 0x75, 0x6c, 0x00, 0x02, 0x00, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00, \n",
            "\t0x10, 0x00, 0x00, 0x00, 0x02, 0xff, 0xff, 0xff, 0x34, 0x00, 0x00, 0x00, \n",
            "\t0x05, 0x00, 0x00, 0x00, 0x0c, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, \n",
            "\t0x74, 0xfe, 0xff, 0xff, 0x1b, 0x00, 0x00, 0x00, 0x73, 0x65, 0x71, 0x75, \n",
            "\t0x65, 0x6e, 0x74, 0x69, 0x61, 0x6c, 0x5f, 0x31, 0x2f, 0x64, 0x65, 0x6e, \n",
            "\t0x73, 0x65, 0x5f, 0x33, 0x2f, 0x4d, 0x61, 0x74, 0x4d, 0x75, 0x6c, 0x00, \n",
            "\t0x02, 0x00, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, \n",
            "\t0x46, 0xff, 0xff, 0xff, 0x50, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, \n",
            "\t0x0c, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0xb8, 0xfe, 0xff, 0xff, \n",
            "\t0x34, 0x00, 0x00, 0x00, 0x73, 0x65, 0x71, 0x75, 0x65, 0x6e, 0x74, 0x69, \n",
            "\t0x61, 0x6c, 0x5f, 0x31, 0x2f, 0x64, 0x65, 0x6e, 0x73, 0x65, 0x5f, 0x35, \n",
            "\t0x2f, 0x42, 0x69, 0x61, 0x73, 0x41, 0x64, 0x64, 0x2f, 0x52, 0x65, 0x61, \n",
            "\t0x64, 0x56, 0x61, 0x72, 0x69, 0x61, 0x62, 0x6c, 0x65, 0x4f, 0x70, 0x2f, \n",
            "\t0x72, 0x65, 0x73, 0x6f, 0x75, 0x72, 0x63, 0x65, 0x00, 0x00, 0x00, 0x00, \n",
            "\t0x01, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0xa2, 0xff, 0xff, 0xff, \n",
            "\t0x50, 0x00, 0x00, 0x00, 0x03, 0x00, 0x00, 0x00, 0x0c, 0x00, 0x00, 0x00, \n",
            "\t0x04, 0x00, 0x00, 0x00, 0x14, 0xff, 0xff, 0xff, 0x34, 0x00, 0x00, 0x00, \n",
            "\t0x73, 0x65, 0x71, 0x75, 0x65, 0x6e, 0x74, 0x69, 0x61, 0x6c, 0x5f, 0x31, \n",
            "\t0x2f, 0x64, 0x65, 0x6e, 0x73, 0x65, 0x5f, 0x34, 0x2f, 0x42, 0x69, 0x61, \n",
            "\t0x73, 0x41, 0x64, 0x64, 0x2f, 0x52, 0x65, 0x61, 0x64, 0x56, 0x61, 0x72, \n",
            "\t0x69, 0x61, 0x62, 0x6c, 0x65, 0x4f, 0x70, 0x2f, 0x72, 0x65, 0x73, 0x6f, \n",
            "\t0x75, 0x72, 0x63, 0x65, 0x00, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, \n",
            "\t0x10, 0x00, 0x00, 0x00, 0x00, 0x00, 0x0e, 0x00, 0x14, 0x00, 0x04, 0x00, \n",
            "\t0x00, 0x00, 0x08, 0x00, 0x0c, 0x00, 0x10, 0x00, 0x0e, 0x00, 0x00, 0x00, \n",
            "\t0x50, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 0x0c, 0x00, 0x00, 0x00, \n",
            "\t0x04, 0x00, 0x00, 0x00, 0x80, 0xff, 0xff, 0xff, 0x34, 0x00, 0x00, 0x00, \n",
            "\t0x73, 0x65, 0x71, 0x75, 0x65, 0x6e, 0x74, 0x69, 0x61, 0x6c, 0x5f, 0x31, \n",
            "\t0x2f, 0x64, 0x65, 0x6e, 0x73, 0x65, 0x5f, 0x33, 0x2f, 0x42, 0x69, 0x61, \n",
            "\t0x73, 0x41, 0x64, 0x64, 0x2f, 0x52, 0x65, 0x61, 0x64, 0x56, 0x61, 0x72, \n",
            "\t0x69, 0x61, 0x62, 0x6c, 0x65, 0x4f, 0x70, 0x2f, 0x72, 0x65, 0x73, 0x6f, \n",
            "\t0x75, 0x72, 0x63, 0x65, 0x00, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, \n",
            "\t0x10, 0x00, 0x00, 0x00, 0x14, 0x00, 0x18, 0x00, 0x04, 0x00, 0x00, 0x00, \n",
            "\t0x08, 0x00, 0x0c, 0x00, 0x10, 0x00, 0x00, 0x00, 0x00, 0x00, 0x14, 0x00, \n",
            "\t0x14, 0x00, 0x00, 0x00, 0x3c, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, \n",
            "\t0x20, 0x00, 0x00, 0x00, 0x18, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, \n",
            "\t0x02, 0x00, 0x00, 0x00, 0xff, 0xff, 0xff, 0xff, 0x01, 0x00, 0x00, 0x00, \n",
            "\t0x04, 0x00, 0x04, 0x00, 0x04, 0x00, 0x00, 0x00, 0x0d, 0x00, 0x00, 0x00, \n",
            "\t0x64, 0x65, 0x6e, 0x73, 0x65, 0x5f, 0x33, 0x5f, 0x69, 0x6e, 0x70, 0x75, \n",
            "\t0x74, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, \n",
            "\t0x01, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00, \n",
            "\t0x00, 0x00, 0x0a, 0x00, 0x0c, 0x00, 0x07, 0x00, 0x00, 0x00, 0x08, 0x00, \n",
            "\t0x0a, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x09, 0x03, 0x00, 0x00, 0x00\n",
            "};\n",
            "const int model_data_len = 2928;\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}